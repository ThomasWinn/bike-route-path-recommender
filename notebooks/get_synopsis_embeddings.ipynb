{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "We ultimately want to fill in manga ratings that are NA or null.\n",
    "\n",
    "\n",
    "We are choosing SBERT (Sentence BERT) as opposed to TFIDF cosine similarity because we want to predict ratings based on synopsis's that are semantically similar. It uses transformer-based architectures, such as BERT, that understand context and relationships between words, enabling it to capture the meaning of a sentence as a whole. TFIDF won't be a good method to use because it primiarily checks for word occurance and frequency which doesn't capture contexts or semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manga_id                 0\n",
      "title                    0\n",
      "type                     0\n",
      "score                21787\n",
      "scored_by                0\n",
      "status                   0\n",
      "volumes              10655\n",
      "chapters             11147\n",
      "start_date            1618\n",
      "end_date              9288\n",
      "members                  0\n",
      "favorites                0\n",
      "sfw                      0\n",
      "approved                 0\n",
      "created_at_before        0\n",
      "updated_at            1819\n",
      "real_start_date       1618\n",
      "real_end_date         9288\n",
      "genres                   0\n",
      "themes                   0\n",
      "demographics             0\n",
      "authors                  0\n",
      "serializations           0\n",
      "synopsis                 0\n",
      "background           33479\n",
      "main_picture            15\n",
      "url                      0\n",
      "title_english        28978\n",
      "title_japanese        1002\n",
      "title_synonyms           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/manga.csv')\n",
    "df = df.dropna(subset=['synopsis'])\n",
    "print(df.isnull().sum())\n",
    "# MODEL = 'all-mpnet-base-v2' # General-purpose, best for applications where accuracy is more important than speed.\n",
    "# MODEL = 'paraphrase-MiniLM-L6-v2' # Balanced Performance. Paraphrase identification, text similarity tasks.\n",
    "MODEL = 'paraphrase-mpnet-base-v2' # High-accuracy paraphrase identification and text similarity tasks. Slightly slower than MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the column containing synopses is named 'synopsis'\n",
    "df = df.dropna(subset=['synopsis'])\n",
    "synopses = df['synopsis'].tolist()\n",
    "\n",
    "# Load a pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer(MODEL) \n",
    "\n",
    "# Compute embeddings for each synopsis\n",
    "embeddings = model.encode(synopses, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can convert it to a numpy array if needed:\n",
    "embeddings_np = embeddings.cpu().numpy()\n",
    "\n",
    "np.savez('../data/embeddings/synopsis_embeddings.npz', embeddings=embeddings_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
