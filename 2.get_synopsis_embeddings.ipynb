{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "We ultimately want to fill in manga ratings that are NA or null.\n",
    "\n",
    "\n",
    "We are choosing SBERT (Sentence BERT) as opposed to TFIDF cosine similarity because we want to predict ratings based on synopsis's that are semantically similar. It uses transformer-based architectures, such as BERT, that understand context and relationships between words, enabling it to capture the meaning of a sentence as a whole. TFIDF won't be a good method to use because it primiarily checks for word occurance and frequency which doesn't capture contexts or semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tpngu\\anaconda3\\envs\\manga-rec\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/modified_manga.csv')\n",
    "# MODEL = 'all-mpnet-base-v2' # General-purpose, best for applications where accuracy is more important than speed.\n",
    "MODEL = 'paraphrase-MiniLM-L6-v2' # Balanced Performance. Paraphrase identification, text similarity tasks.\n",
    "# MODEL = 'paraphrase-mpnet-base-v2' # High-accuracy paraphrase identification and text similarity tasks. Slightly slower than MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tpngu\\anaconda3\\envs\\manga-rec\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tpngu\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Assuming the column containing synopses is named 'synopsis'\n",
    "synopses = df['synopsis'].tolist()\n",
    "\n",
    "# Load a pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer(MODEL) \n",
    "\n",
    "# Compute embeddings for each synopsis\n",
    "embeddings = model.encode(synopses, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, 'embeddings' is a tensor containing the sentence embeddings for each synopsis\n",
    "# You can convert it to a numpy array if needed:\n",
    "embeddings_np = embeddings.cpu().numpy()\n",
    "\n",
    "# Convert each embedding to a comma-separated string\n",
    "embeddings_str = [','.join(map(str, embedding)) for embedding in embeddings_np]\n",
    "\n",
    "# Add the embeddings back to the dataframe\n",
    "df['embeddings'] = embeddings_str\n",
    "\n",
    "# # If you want to add the embeddings back to the dataframe and save it\n",
    "# df['embeddings'] = list(embeddings_np)\n",
    "\n",
    "fname = 'dataset/manga_dataset_with_embeddings_' + MODEL + '.csv'\n",
    "\n",
    "df.to_csv(fname, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
